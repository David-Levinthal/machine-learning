# machine-learning
repository for notes and data from machine learning studies


The rapid rise in the successful use of machine learning (ML) algorithms raises the issue of what an optimal architecture for ML acceleration should look like. To help such an investigation, some discussion of the numerical operations of the dominant ML networks is required as a starting point. What follows will hopefully shed some light on this.
The two dominant ML network architectures, at the time of this writing, are convolutional neural networks (CNNS) and recurrent neural networks (RNNs). CNNs are currently dominantly used for 2-dimensional image recognition like surveillance, self-driving cars and drones and tagging which selfies have a cat. RNNs are dominantly used for sequential input (text, voice) recognition and manipulation respectively. As convolution is the basis for FFTs it is not surprising that CNNs are also useful in decomposing speech waveform patterns into phonemes, from which they can then be processed by RNNs into language and processed. These two techniques represent the dominant approaches in DNN applications.

https://github.com/David-Levinthal/machine-learning/blob/master/Introduction%20to%20machine%20learning%20algorithms.pdf
